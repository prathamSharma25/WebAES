{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c67412",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1><b>Word Embedding Based Answer Evaluation System for Online Assessments (WebAES)</b></h1>\n",
    "<h3>A smart system to automate the process of answer evaluation in online assessments.</h3>\n",
    "<h5>LDA Model for topic extraction</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bb9e573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For training LDA model using text8 corpus\n",
    "from gensim import corpora, models\n",
    "import gensim.downloader as api\n",
    "\n",
    "# To perform text pre-processing\n",
    "import string\n",
    "\n",
    "# Natural Language Toolkit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set of stopwords in English\n",
    "en_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "712a31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text8 corpus and convert to list of documents\n",
    "text8_corpus = api.load('text8')\n",
    "text8_corpus = [doc for doc in text8_corpus]\n",
    "\n",
    "# List containing list of tokens from each document of text8 corpus\n",
    "list_of_list_of_tokens = []\n",
    "\n",
    "# For each document in the text8 corpus\n",
    "for i in range(len(text8_corpus)):\n",
    "    # Remove stopwords from each document\n",
    "    text8_corpus[i] = [w for w in text8_corpus[i] if w not in en_stopwords]\n",
    "    \n",
    "    # Add list of tokens for document to list of list of tokens\n",
    "    list_of_list_of_tokens.append(text8_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79211881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 46s\n"
     ]
    }
   ],
   "source": [
    "# Create a text corpus using list of list of tokens\n",
    "dictionary_LDA = corpora.Dictionary(list_of_list_of_tokens)\n",
    "corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in list_of_list_of_tokens]\n",
    "\n",
    "# LDA model to extract 100 topics from the text corpus\n",
    "num_topics = 100\n",
    "%time lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary_LDA, passes=4, alpha=[0.01]*num_topics, eta=[0.01]*len(dictionary_LDA.keys()))\n",
    "\n",
    "# Save trained LDA model\n",
    "lda_model.save('WebAES_LDA_Model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed7d30a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously trained LDA model\n",
    "lda_model =  models.LdaModel.load('WebAES_LDA_Model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0950c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All topics:  [(77, 0.99262595)]\n",
      "Index of topic with max. probability:  77\n"
     ]
    }
   ],
   "source": [
    "# Pre-process text\n",
    "new_text = 'The different domains of Artificial Intelligence include machine learning, neural networks, robotics, expert systems, fuzzy logic, and natural language processing. Machine Learning is the science of getting computers to act by feeding them data so that they can learn a few tricks on their own, without being explicitly programmed to do so. Neural Networks are a set of algorithms and techniques, modelled in accordance with the human brain. Neural Networks are designed to solve complex and advanced machine learning problems. Robotics is a subset of AI, which includes different branches and application of robots. These Robots are artificial agents acting in a real-world environment. An AI Robot works by manipulating the objects in its surrounding, by perceiving, moving and taking relevant actions. An expert system is a computer system that mimics the decision-making ability of a human. It is a computer program that uses artificial intelligence (AI) technologies to simulate the judgment and behaviour of a human or an organization that has expert knowledge and experience in a particular field. Fuzzy logic is an approach to computing based on “degrees of truth” rather than the usual “true or false” (1 or 0) Boolean logic on which the modern computer is based. Fuzzy logic Systems can take imprecise, distorted, noisy input information. Natural Language Processing (NLP) refers to the Artificial Intelligence method that analyses natural human language to derive useful insights in order to solve problems.'\n",
    "# Remove punctuations\n",
    "new_text = new_text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "# Remove stopwords\n",
    "new_text = ' '.join([w for w in new_text.split() if not w.lower() in en_stopwords])\n",
    "# Create list of tokens\n",
    "new_text_tokens = new_text.split()\n",
    "\n",
    "# Get probability of belonging to each topic using LDA model\n",
    "topic_probs = lda_model[dictionary_LDA.doc2bow(new_text_tokens)]\n",
    "\n",
    "# Display all topics and their probability\n",
    "print('All topics: ', topic_probs)\n",
    "\n",
    "# Select index of topic with maximum topic probability\n",
    "max_prob = 0\n",
    "max_prob_topic = 0\n",
    "for topic in topic_probs:\n",
    "    topic_index, topic_prob = topic[0], topic[1]\n",
    "    if topic_prob > max_prob:\n",
    "        max_prob = topic_prob\n",
    "        max_prob_topic = topic_index\n",
    "\n",
    "# Display topic index with maximum topic probability\n",
    "print('Index of topic with max. probability: ', max_prob_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "858d8c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All topics:  [(77, 0.94482034)]\n",
      "Index of topic with max. probability:  77\n"
     ]
    }
   ],
   "source": [
    "# Pre-process text\n",
    "new_text = 'The different domains of Artificial Intelligence include machine learning, neural networks, robotics, expert systems, fuzzy logic, and natural language processing.'\n",
    "# Remove punctuations\n",
    "new_text = new_text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "# Remove stopwords\n",
    "new_text = ' '.join([w for w in new_text.split() if not w.lower() in en_stopwords])\n",
    "# Create list of tokens\n",
    "new_text_tokens = new_text.split()\n",
    "\n",
    "# Get probability of belonging to each topic using LDA model\n",
    "topic_probs = lda_model[dictionary_LDA.doc2bow(new_text_tokens)]\n",
    "\n",
    "# Display all topics and their probability\n",
    "print('All topics: ', topic_probs)\n",
    "\n",
    "# Select index of topic with maximum topic probability\n",
    "max_prob = 0\n",
    "max_prob_topic = 0\n",
    "for topic in topic_probs:\n",
    "    topic_index, topic_prob = topic[0], topic[1]\n",
    "    if topic_prob > max_prob:\n",
    "        max_prob = topic_prob\n",
    "        max_prob_topic = topic_index\n",
    "\n",
    "# Display topic index with maximum topic probability\n",
    "print('Index of topic with max. probability: ', max_prob_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d083555d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All topics:  [(31, 0.9009669)]\n",
      "Index of topic with max. probability:  31\n"
     ]
    }
   ],
   "source": [
    "# Pre-process text\n",
    "new_text = 'Human Computer Interaction is the study of the interaction between human users and computer systems.'\n",
    "# Remove punctuations\n",
    "new_text = new_text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "# Remove stopwords\n",
    "new_text = ' '.join([w for w in new_text.split() if not w.lower() in en_stopwords])\n",
    "# Create list of tokens\n",
    "new_text_tokens = new_text.split()\n",
    "\n",
    "# Get probability of belonging to each topic using LDA model\n",
    "topic_probs = lda_model[dictionary_LDA.doc2bow(new_text_tokens)]\n",
    "\n",
    "# Display all topics and their probability\n",
    "print('All topics: ', topic_probs)\n",
    "\n",
    "# Select index of topic with maximum topic probability\n",
    "max_prob = 0\n",
    "max_prob_topic = 0\n",
    "for topic in topic_probs:\n",
    "    topic_index, topic_prob = topic[0], topic[1]\n",
    "    if topic_prob > max_prob:\n",
    "        max_prob = topic_prob\n",
    "        max_prob_topic = topic_index\n",
    "\n",
    "# Display topic index with maximum topic probability\n",
    "print('Index of topic with max. probability: ', max_prob_topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
